---
title: Untitled
subtitle: A slightly longer title
format:
  360-analysis-html: default
author: James Goldie
date: last-modified
code-fold: true
---

```{r}
#| label: setup
library(tidyverse)
library(ecmwfr)
library(sf)
library(ClimateOperators)
library(rgeoboundaries)
library(here)
```

# Retrieving the ERA5 data

The ECMWF API requires authentication. When rendering this document, ensure a `.Renviron` file is present with an environment variable. The variable name should be the username prefixed with `ecmwfr_cds:`, and the value should be the key:

```
ecmwfr_cds:[user]=[key]
```

We're going to download five years at a time.

:::{.callout-warning}
Downloading this data takes **a lot of disk space** â€” about 450 GB. If you need a separate disk for this data, mount an external drive to `/data/cds/raw` inside the container by adding this to `devcontainer.json` (here `/path/to/external/storage` is the path to the external storage):

```json
"mounts": [
  "source=/path/to/external/storage,target=/workspaces/report-heatstress-asia/data/cds/raw,type=bind,consistency=cached"
	]
```

It also takes a long time to download from CDS, as there is server-side processing required for each annual file (about 12 hours, by my testing).

It may make sense to run this code interactively, but as it only downloads missing files, you should be fine re-running it if there's a failure.
:::

```{r}
#| label: authenticate
cds_keys <-
  here(".Renviron") |>
  readLines() |>
  keep(str_starts, "ecmwfr_cds:")

# check exactly one key
stopifnot(
    "There should be exactly one environment variable of the form `ecmwfr_cds:[user]=[key]`" =
  length(cds_keys) == 1)

# isolate the username
username <-
  cds_keys |>
  str_split("=") |>
  unlist() |>
  pluck(1) |>
  str_remove("ecmwfr_cds:")
```

```{r}
#| label: download

# which years do we need to download?
raw_dir <- here("data", "cds", "raw", "daily")
dir.create(raw_dir, recursive = TRUE, showWarnings = FALSE)

build_request <- function(start) {

  # request template (for parameters that don't change)
  list(
    dataset_short_name = "derived-utci-historical",
    product_type = "consolidated_dataset",
    version = "1_1",
    variable = "universal_thermal_climate_index",
    year = start,
    month = c(
      "01", "02", "03",
      "04", "05", "06",
      "07", "08", "09",
      "10", "11", "12"),
    day = c(
      "01", "02", "03",
      "04", "05", "06",
      "07", "08", "09",
      "10", "11", "12",
      "13", "14", "15",
      "16", "17", "18",
      "19", "20", "21",
      "22", "23", "24",
      "25", "26", "27",
      "28", "29", "30",
      "31"),
    format = "zip",
    target = paste0(start, ".zip"))
}

tibble(
  start = 1990:year(Sys.Date()),
  req_obj = map(start, build_request)) |>
  mutate(fname = map_chr(req_obj, ~ pluck(.x, "target"))) ->
seq_files

# temp for downloading
seq_files |> filter(between(start, 1994, 2004)) -> missing_files

seq_files |>
  filter(!file.exists(file.path(raw_dir, fname))) |>
  arrange(desc(start)) ->
missing_files

# make a separate request for each missing sequence (if there are any)
# (this might take a long, *long* time if there are many years to retrieve!)
if (length(missing_files) > 0) {
  wf_request_batch(
    request_list = missing_files$req_obj,
    user         = username,
    path         = raw_dir)
}
```

# Unzip dailies

Each request is a ZIP file of one year containing NetCDF files for each day in that year. The good news is that there's no compression, so once the file is successfully unzipped we can delete the archive.

```{r}
#| label: unzip
unzip_and_unlink <- function(path) {
  unzip(
    here("data", "cds", "raw", "daily", path),
    exdir = here("data", "cds", "raw", "daily"))
  unlink(here("data", "cds", "raw", "daily", path))
}

# unzip and delete the zip files
seq_files |>
  filter(file.exists(file.path(raw_dir, fname))) |>
  # slice(1:2) |>
  pull(fname) |>
  walk(unzip_and_unlink, .progress = TRUE)
```

# Calculating thresholds

Once that's done, we want to time merge each of the daily files for the year and tally up the number of days above a certain UTCI threshold. Since the requests already correspond to individual years, it's pretty easy to isolate the NC files for a year (although they're also named `ECMWF_utci_YYYYMMDD_v1.1_con.nc`, so you can grep them too!).

The thresholds we'll use are based on [Copernicus's thresholds](https://urbansis.eu/universal-thermal-climate-index) (also in their [thermofeel package docs](https://thermofeel.readthedocs.io/en/latest/guide/utci.html)):

* \> +46: extreme heat stress
* +38 to +46: very strong heat stress
* +32 to +38: strong heat stress
* +26 to +32: moderate heat stress
* +9 to +26: no thermal stress
* +9 to 0: slight cold stress
* 0 to -13: moderate cold stress
* -13 to -27: strong cold stress
* -27 to -40: very strong cold stress
* < -40: extreme cold stress.

```{r}
#| label: join-and-count-fn
join_and_count <- function(year, lower, upper) {

  # get a list of the dailies nc files for the specified year
  tibble(
    path = list.files(here("data", "cds", "raw", "daily"),
      pattern = glob2rx("*.nc"),
      full.names = TRUE)) |>
    mutate(year = as.integer(str_extract(basename(path), "\\d{4}"))) |>
    filter(year == year) ->
  dailies

  # thresholds are [lower, upper).
  # (temperatures are in kelvin so there are no negative values)
  if (lower == -Inf) {
    compare_op <- ssl(
      csl("setctomiss", 0), 
      csl("-ltc", upper + 273.15))
  } else if (upper == Inf) {
    compare_op <- ssl(
      csl("setctomiss", 0), 
      csl("-gec", lower + 273.15))
  } else {
    compare_op <- ssl(
      csl("setctomiss", 0),
      csl("-ltc", upper + 273.15),
      csl("setctomiss", 0),
      csl("-gec", lower + 273.15)
    )
  }

  cdo("-L", "-O",
    # count the days
    "-timcumsum",
    # drop days outside the thresholds
    compare_op,
    "-mergetime",
    paste(dailies$path, collapse = " "),
    here("data", "cds", "raw", "annual",
      paste0(year, "_", lower, "_to_", upper, ".nc"))
  )
}
```

```{r}
#| label: join-and-count
dir.create(here("data", "cds", "raw", "annual"), showWarnings = FALSE)

# here are the utci thresholds we want to look at
tibble(lower = c(46, 38, 32, 26, 9, 0, -13, -27, -40, -Inf)) |>
  mutate(upper = lag(lower)) |>
  replace_na(list(upper = Inf)) ->
thresholds

# get the available years from the dailies,
# then cross them with the thresholds
here("data", "cds", "raw", "daily") |>
  list.files(pattern = glob2rx(paste0("*.nc"))) |>
  str_replace_all(c("ECMWF_utci_" = "", "_v1.1_con.nc" = "")) |>
  str_sub(1, 4) |>
  unique() |>
  (\(year) expand_grid(year, thresholds))() ->
year_thresh_combos

# finally, run join_and_count on each year/threshold combo
year_thresh_combos |> pwalk(join_and_count, .progress = TRUE)
```

# Boundaries

Finally, we need to get field averages for each region.

Let's start with the boundaries of countries using `{rgeoboundaries}`, as well as Indian states:

```{r}
#| label: get-boundaries
boundaries_countries <- gb_adm0(type = "simplified")
boundaries_india     <- gb_adm1("ind", type = "simplified")
```

Now we'll use `{exactextractr}` to get the field averages for each feature, from each year/threshold .nc file:


```{r}
#| label: calc-field-averages
```